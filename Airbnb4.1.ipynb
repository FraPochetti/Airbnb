{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import scipy.sparse as sp\n",
    "from mylib.scoring import ndcg_at_k, mean_ndcg\n",
    "from mylib.preprocess import make_sessions_features, make_user_features\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join('data', 'test_users.csv'), header=0, parse_dates=[1,2,3])\n",
    "train = pd.read_csv(os.path.join('data', 'train_users_2.csv'), header=0, parse_dates=[1,2,3])\n",
    "df_sessions = pd.read_csv(\"data/sessions.csv\", encoding='utf8')\n",
    "from mylib.scoring import ndcg_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_features_train, user_features_test, y, le = make_user_features(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session_features_train = make_sessions_features(user_features_train, df_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session_features_test = make_sessions_features(user_features_test, df_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = final[:train.shape[0]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(session_features_train, \n",
    "                                                    y, test_size=0.33, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
       " array([  178,   471,   350,   742,  1658,   767,   936, 41099,   251,\n",
       "           72, 20584,  3331]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb.DMatrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until train error hasn't decreased in 10 rounds.\n",
      "[0]\ttest-mlogloss:2.411685\ttrain-mlogloss:2.444891\n",
      "[1]\ttest-mlogloss:2.366699\ttrain-mlogloss:2.415701\n",
      "[2]\ttest-mlogloss:2.337090\ttrain-mlogloss:2.391770\n",
      "[3]\ttest-mlogloss:2.314797\ttrain-mlogloss:2.370515\n",
      "[4]\ttest-mlogloss:2.295016\ttrain-mlogloss:2.350060\n",
      "[5]\ttest-mlogloss:2.280881\ttrain-mlogloss:2.334300\n",
      "[6]\ttest-mlogloss:2.265913\ttrain-mlogloss:2.319496\n",
      "[7]\ttest-mlogloss:2.254261\ttrain-mlogloss:2.305061\n",
      "[8]\ttest-mlogloss:2.246562\ttrain-mlogloss:2.291126\n",
      "[9]\ttest-mlogloss:2.238514\ttrain-mlogloss:2.279339\n",
      "[10]\ttest-mlogloss:2.232111\ttrain-mlogloss:2.267637\n",
      "[11]\ttest-mlogloss:2.222831\ttrain-mlogloss:2.253698\n",
      "[12]\ttest-mlogloss:2.217057\ttrain-mlogloss:2.242778\n",
      "[13]\ttest-mlogloss:2.212477\ttrain-mlogloss:2.230071\n",
      "[14]\ttest-mlogloss:2.206125\ttrain-mlogloss:2.220570\n",
      "[15]\ttest-mlogloss:2.200396\ttrain-mlogloss:2.210973\n",
      "[16]\ttest-mlogloss:2.197464\ttrain-mlogloss:2.204733\n",
      "[17]\ttest-mlogloss:2.193844\ttrain-mlogloss:2.191894\n",
      "[18]\ttest-mlogloss:2.188107\ttrain-mlogloss:2.183152\n",
      "[19]\ttest-mlogloss:2.184801\ttrain-mlogloss:2.175486\n",
      "[20]\ttest-mlogloss:2.181758\ttrain-mlogloss:2.167184\n",
      "[21]\ttest-mlogloss:2.178247\ttrain-mlogloss:2.161781\n",
      "[22]\ttest-mlogloss:2.174667\ttrain-mlogloss:2.154276\n",
      "[23]\ttest-mlogloss:2.171360\ttrain-mlogloss:2.147817\n",
      "[24]\ttest-mlogloss:2.168292\ttrain-mlogloss:2.141452\n",
      "[25]\ttest-mlogloss:2.165915\ttrain-mlogloss:2.135785\n",
      "[26]\ttest-mlogloss:2.163588\ttrain-mlogloss:2.128929\n",
      "[27]\ttest-mlogloss:2.160632\ttrain-mlogloss:2.123369\n",
      "[28]\ttest-mlogloss:2.159105\ttrain-mlogloss:2.117759\n",
      "[29]\ttest-mlogloss:2.156297\ttrain-mlogloss:2.110617\n",
      "[30]\ttest-mlogloss:2.154217\ttrain-mlogloss:2.104173\n",
      "[31]\ttest-mlogloss:2.152971\ttrain-mlogloss:2.098607\n",
      "[32]\ttest-mlogloss:2.149350\ttrain-mlogloss:2.091527\n",
      "[33]\ttest-mlogloss:2.147053\ttrain-mlogloss:2.087303\n",
      "[34]\ttest-mlogloss:2.145164\ttrain-mlogloss:2.083313\n",
      "[35]\ttest-mlogloss:2.142581\ttrain-mlogloss:2.078599\n",
      "[36]\ttest-mlogloss:2.140934\ttrain-mlogloss:2.072189\n",
      "[37]\ttest-mlogloss:2.139430\ttrain-mlogloss:2.067868\n",
      "[38]\ttest-mlogloss:2.137007\ttrain-mlogloss:2.062989\n",
      "[39]\ttest-mlogloss:2.134329\ttrain-mlogloss:2.058511\n",
      "[40]\ttest-mlogloss:2.132789\ttrain-mlogloss:2.053325\n",
      "[41]\ttest-mlogloss:2.130672\ttrain-mlogloss:2.049327\n",
      "[42]\ttest-mlogloss:2.129196\ttrain-mlogloss:2.044630\n",
      "[43]\ttest-mlogloss:2.128043\ttrain-mlogloss:2.039057\n",
      "[44]\ttest-mlogloss:2.126081\ttrain-mlogloss:2.035206\n",
      "[45]\ttest-mlogloss:2.124393\ttrain-mlogloss:2.028730\n",
      "[46]\ttest-mlogloss:2.123399\ttrain-mlogloss:2.025332\n",
      "[47]\ttest-mlogloss:2.121763\ttrain-mlogloss:2.021944\n",
      "[48]\ttest-mlogloss:2.120097\ttrain-mlogloss:2.017686\n",
      "[49]\ttest-mlogloss:2.118834\ttrain-mlogloss:2.013400\n",
      "[50]\ttest-mlogloss:2.117596\ttrain-mlogloss:2.008767\n",
      "[51]\ttest-mlogloss:2.114391\ttrain-mlogloss:2.003234\n",
      "[52]\ttest-mlogloss:2.112849\ttrain-mlogloss:1.999699\n",
      "[53]\ttest-mlogloss:2.111806\ttrain-mlogloss:1.996090\n",
      "[54]\ttest-mlogloss:2.110508\ttrain-mlogloss:1.990952\n",
      "[55]\ttest-mlogloss:2.109255\ttrain-mlogloss:1.987607\n",
      "[56]\ttest-mlogloss:2.107944\ttrain-mlogloss:1.984031\n",
      "[57]\ttest-mlogloss:2.106445\ttrain-mlogloss:1.980686\n",
      "[58]\ttest-mlogloss:2.104698\ttrain-mlogloss:1.977562\n",
      "[59]\ttest-mlogloss:2.102613\ttrain-mlogloss:1.972659\n",
      "[60]\ttest-mlogloss:2.101012\ttrain-mlogloss:1.969827\n",
      "[61]\ttest-mlogloss:2.099953\ttrain-mlogloss:1.967385\n",
      "[62]\ttest-mlogloss:2.098831\ttrain-mlogloss:1.964932\n",
      "[63]\ttest-mlogloss:2.098063\ttrain-mlogloss:1.962315\n",
      "[64]\ttest-mlogloss:2.096827\ttrain-mlogloss:1.958880\n",
      "[65]\ttest-mlogloss:2.095184\ttrain-mlogloss:1.956614\n",
      "[66]\ttest-mlogloss:2.094051\ttrain-mlogloss:1.953599\n",
      "[67]\ttest-mlogloss:2.093240\ttrain-mlogloss:1.951478\n",
      "[68]\ttest-mlogloss:2.091636\ttrain-mlogloss:1.949248\n",
      "[69]\ttest-mlogloss:2.090595\ttrain-mlogloss:1.946050\n",
      "[70]\ttest-mlogloss:2.089301\ttrain-mlogloss:1.942377\n",
      "[71]\ttest-mlogloss:2.088495\ttrain-mlogloss:1.938562\n",
      "[72]\ttest-mlogloss:2.087441\ttrain-mlogloss:1.935403\n",
      "[73]\ttest-mlogloss:2.086596\ttrain-mlogloss:1.932133\n",
      "[74]\ttest-mlogloss:2.085792\ttrain-mlogloss:1.929987\n",
      "[75]\ttest-mlogloss:2.084724\ttrain-mlogloss:1.928207\n",
      "[76]\ttest-mlogloss:2.083794\ttrain-mlogloss:1.926145\n",
      "[77]\ttest-mlogloss:2.082838\ttrain-mlogloss:1.923377\n",
      "[78]\ttest-mlogloss:2.081900\ttrain-mlogloss:1.920583\n",
      "[79]\ttest-mlogloss:2.080771\ttrain-mlogloss:1.918070\n",
      "[80]\ttest-mlogloss:2.079114\ttrain-mlogloss:1.914876\n",
      "[81]\ttest-mlogloss:2.077726\ttrain-mlogloss:1.913024\n",
      "[82]\ttest-mlogloss:2.076673\ttrain-mlogloss:1.909475\n",
      "[83]\ttest-mlogloss:2.075164\ttrain-mlogloss:1.907245\n",
      "[84]\ttest-mlogloss:2.074397\ttrain-mlogloss:1.904300\n",
      "[85]\ttest-mlogloss:2.073195\ttrain-mlogloss:1.901421\n",
      "[86]\ttest-mlogloss:2.071798\ttrain-mlogloss:1.896661\n",
      "[87]\ttest-mlogloss:2.070364\ttrain-mlogloss:1.893905\n",
      "[88]\ttest-mlogloss:2.069314\ttrain-mlogloss:1.891581\n",
      "[89]\ttest-mlogloss:2.068429\ttrain-mlogloss:1.889972\n",
      "[90]\ttest-mlogloss:2.067105\ttrain-mlogloss:1.887045\n",
      "[91]\ttest-mlogloss:2.066476\ttrain-mlogloss:1.883866\n",
      "[92]\ttest-mlogloss:2.065525\ttrain-mlogloss:1.881867\n",
      "[93]\ttest-mlogloss:2.064898\ttrain-mlogloss:1.879103\n",
      "[94]\ttest-mlogloss:2.064149\ttrain-mlogloss:1.877737\n",
      "[95]\ttest-mlogloss:2.063102\ttrain-mlogloss:1.875987\n",
      "[96]\ttest-mlogloss:2.062331\ttrain-mlogloss:1.874496\n",
      "[97]\ttest-mlogloss:2.061304\ttrain-mlogloss:1.872032\n",
      "[98]\ttest-mlogloss:2.060376\ttrain-mlogloss:1.869237\n",
      "[99]\ttest-mlogloss:2.059087\ttrain-mlogloss:1.866539\n",
      "[100]\ttest-mlogloss:2.058205\ttrain-mlogloss:1.864279\n",
      "[101]\ttest-mlogloss:2.057360\ttrain-mlogloss:1.860404\n",
      "[102]\ttest-mlogloss:2.056539\ttrain-mlogloss:1.857214\n",
      "[103]\ttest-mlogloss:2.055759\ttrain-mlogloss:1.854132\n",
      "[104]\ttest-mlogloss:2.055098\ttrain-mlogloss:1.851411\n",
      "[105]\ttest-mlogloss:2.054220\ttrain-mlogloss:1.848885\n",
      "[106]\ttest-mlogloss:2.053430\ttrain-mlogloss:1.846754\n",
      "[107]\ttest-mlogloss:2.052592\ttrain-mlogloss:1.845128\n",
      "[108]\ttest-mlogloss:2.051588\ttrain-mlogloss:1.842984\n",
      "[109]\ttest-mlogloss:2.050952\ttrain-mlogloss:1.840969\n",
      "[110]\ttest-mlogloss:2.050399\ttrain-mlogloss:1.839154\n",
      "[111]\ttest-mlogloss:2.049384\ttrain-mlogloss:1.837141\n",
      "[112]\ttest-mlogloss:2.048074\ttrain-mlogloss:1.834104\n",
      "[113]\ttest-mlogloss:2.047273\ttrain-mlogloss:1.831970\n",
      "[114]\ttest-mlogloss:2.046567\ttrain-mlogloss:1.829966\n",
      "[115]\ttest-mlogloss:2.045857\ttrain-mlogloss:1.826592\n",
      "[116]\ttest-mlogloss:2.045342\ttrain-mlogloss:1.824783\n",
      "[117]\ttest-mlogloss:2.044137\ttrain-mlogloss:1.820885\n",
      "[118]\ttest-mlogloss:2.043178\ttrain-mlogloss:1.818719\n",
      "[119]\ttest-mlogloss:2.042171\ttrain-mlogloss:1.817317\n",
      "[120]\ttest-mlogloss:2.041418\ttrain-mlogloss:1.815378\n",
      "[121]\ttest-mlogloss:2.040317\ttrain-mlogloss:1.813477\n",
      "[122]\ttest-mlogloss:2.039407\ttrain-mlogloss:1.812084\n",
      "[123]\ttest-mlogloss:2.038420\ttrain-mlogloss:1.808806\n",
      "[124]\ttest-mlogloss:2.037595\ttrain-mlogloss:1.806573\n",
      "[125]\ttest-mlogloss:2.036953\ttrain-mlogloss:1.803995\n",
      "[126]\ttest-mlogloss:2.036487\ttrain-mlogloss:1.802195\n",
      "[127]\ttest-mlogloss:2.035796\ttrain-mlogloss:1.800913\n",
      "[128]\ttest-mlogloss:2.035295\ttrain-mlogloss:1.799080\n",
      "[129]\ttest-mlogloss:2.034705\ttrain-mlogloss:1.797404\n",
      "[130]\ttest-mlogloss:2.034054\ttrain-mlogloss:1.796026\n",
      "[131]\ttest-mlogloss:2.033269\ttrain-mlogloss:1.794317\n",
      "[132]\ttest-mlogloss:2.032188\ttrain-mlogloss:1.792191\n",
      "[133]\ttest-mlogloss:2.031357\ttrain-mlogloss:1.791216\n",
      "[134]\ttest-mlogloss:2.030937\ttrain-mlogloss:1.789365\n",
      "[135]\ttest-mlogloss:2.030389\ttrain-mlogloss:1.788246\n",
      "[136]\ttest-mlogloss:2.029881\ttrain-mlogloss:1.785577\n",
      "[137]\ttest-mlogloss:2.028868\ttrain-mlogloss:1.783185\n",
      "[138]\ttest-mlogloss:2.027591\ttrain-mlogloss:1.781531\n",
      "[139]\ttest-mlogloss:2.026574\ttrain-mlogloss:1.779480\n",
      "[140]\ttest-mlogloss:2.026027\ttrain-mlogloss:1.777880\n",
      "[141]\ttest-mlogloss:2.025384\ttrain-mlogloss:1.776392\n",
      "[142]\ttest-mlogloss:2.024688\ttrain-mlogloss:1.774854\n",
      "[143]\ttest-mlogloss:2.024157\ttrain-mlogloss:1.773219\n",
      "[144]\ttest-mlogloss:2.023229\ttrain-mlogloss:1.771425\n",
      "[145]\ttest-mlogloss:2.022410\ttrain-mlogloss:1.767424\n",
      "[146]\ttest-mlogloss:2.021751\ttrain-mlogloss:1.764114\n",
      "[147]\ttest-mlogloss:2.021141\ttrain-mlogloss:1.761310\n",
      "[148]\ttest-mlogloss:2.020259\ttrain-mlogloss:1.759235\n",
      "[149]\ttest-mlogloss:2.019661\ttrain-mlogloss:1.757989\n",
      "[150]\ttest-mlogloss:2.019109\ttrain-mlogloss:1.755537\n",
      "[151]\ttest-mlogloss:2.018276\ttrain-mlogloss:1.753619\n",
      "[152]\ttest-mlogloss:2.017038\ttrain-mlogloss:1.751758\n",
      "[153]\ttest-mlogloss:2.016458\ttrain-mlogloss:1.748664\n",
      "[154]\ttest-mlogloss:2.015707\ttrain-mlogloss:1.746882\n",
      "[155]\ttest-mlogloss:2.014898\ttrain-mlogloss:1.744172\n",
      "[156]\ttest-mlogloss:2.014138\ttrain-mlogloss:1.742418\n",
      "[157]\ttest-mlogloss:2.013493\ttrain-mlogloss:1.740861\n",
      "[158]\ttest-mlogloss:2.012820\ttrain-mlogloss:1.739230\n",
      "[159]\ttest-mlogloss:2.012161\ttrain-mlogloss:1.737668\n",
      "[160]\ttest-mlogloss:2.011038\ttrain-mlogloss:1.735320\n",
      "[161]\ttest-mlogloss:2.010343\ttrain-mlogloss:1.733341\n",
      "[162]\ttest-mlogloss:2.009689\ttrain-mlogloss:1.731987\n",
      "[163]\ttest-mlogloss:2.009017\ttrain-mlogloss:1.730704\n",
      "[164]\ttest-mlogloss:2.008299\ttrain-mlogloss:1.728547\n",
      "[165]\ttest-mlogloss:2.007789\ttrain-mlogloss:1.727138\n",
      "[166]\ttest-mlogloss:2.006988\ttrain-mlogloss:1.726307\n",
      "[167]\ttest-mlogloss:2.006380\ttrain-mlogloss:1.723997\n",
      "[168]\ttest-mlogloss:2.005730\ttrain-mlogloss:1.722158\n",
      "[169]\ttest-mlogloss:2.005177\ttrain-mlogloss:1.720723\n",
      "[170]\ttest-mlogloss:2.004373\ttrain-mlogloss:1.718305\n",
      "[171]\ttest-mlogloss:2.003961\ttrain-mlogloss:1.717106\n",
      "[172]\ttest-mlogloss:2.003232\ttrain-mlogloss:1.715182\n",
      "[173]\ttest-mlogloss:2.002620\ttrain-mlogloss:1.714211\n",
      "[174]\ttest-mlogloss:2.002186\ttrain-mlogloss:1.712576\n",
      "[175]\ttest-mlogloss:2.001597\ttrain-mlogloss:1.709936\n",
      "[176]\ttest-mlogloss:2.001231\ttrain-mlogloss:1.707475\n",
      "[177]\ttest-mlogloss:2.000568\ttrain-mlogloss:1.705131\n",
      "[178]\ttest-mlogloss:2.000071\ttrain-mlogloss:1.703619\n",
      "[179]\ttest-mlogloss:1.999373\ttrain-mlogloss:1.702255\n",
      "[180]\ttest-mlogloss:1.998893\ttrain-mlogloss:1.701265\n",
      "[181]\ttest-mlogloss:1.998429\ttrain-mlogloss:1.699923\n",
      "[182]\ttest-mlogloss:1.997756\ttrain-mlogloss:1.697164\n",
      "[183]\ttest-mlogloss:1.996886\ttrain-mlogloss:1.694687\n",
      "[184]\ttest-mlogloss:1.996253\ttrain-mlogloss:1.692404\n",
      "[185]\ttest-mlogloss:1.995817\ttrain-mlogloss:1.690868\n",
      "[186]\ttest-mlogloss:1.995067\ttrain-mlogloss:1.689117\n",
      "[187]\ttest-mlogloss:1.994218\ttrain-mlogloss:1.687298\n",
      "[188]\ttest-mlogloss:1.993514\ttrain-mlogloss:1.685229\n",
      "[189]\ttest-mlogloss:1.992758\ttrain-mlogloss:1.683821\n",
      "[190]\ttest-mlogloss:1.992082\ttrain-mlogloss:1.681904\n",
      "[191]\ttest-mlogloss:1.991652\ttrain-mlogloss:1.679578\n",
      "[192]\ttest-mlogloss:1.991137\ttrain-mlogloss:1.677157\n",
      "[193]\ttest-mlogloss:1.990863\ttrain-mlogloss:1.675761\n",
      "[194]\ttest-mlogloss:1.990286\ttrain-mlogloss:1.674252\n",
      "[195]\ttest-mlogloss:1.989557\ttrain-mlogloss:1.672395\n",
      "[196]\ttest-mlogloss:1.988619\ttrain-mlogloss:1.671042\n",
      "[197]\ttest-mlogloss:1.987981\ttrain-mlogloss:1.669037\n",
      "[198]\ttest-mlogloss:1.987349\ttrain-mlogloss:1.666252\n",
      "[199]\ttest-mlogloss:1.986828\ttrain-mlogloss:1.664639\n",
      "[200]\ttest-mlogloss:1.986267\ttrain-mlogloss:1.663400\n",
      "[201]\ttest-mlogloss:1.985481\ttrain-mlogloss:1.661858\n",
      "[202]\ttest-mlogloss:1.985113\ttrain-mlogloss:1.660589\n",
      "[203]\ttest-mlogloss:1.984671\ttrain-mlogloss:1.658375\n",
      "[204]\ttest-mlogloss:1.984249\ttrain-mlogloss:1.657637\n",
      "[205]\ttest-mlogloss:1.983914\ttrain-mlogloss:1.655800\n",
      "[206]\ttest-mlogloss:1.983688\ttrain-mlogloss:1.654316\n",
      "[207]\ttest-mlogloss:1.983090\ttrain-mlogloss:1.652013\n",
      "[208]\ttest-mlogloss:1.982438\ttrain-mlogloss:1.648642\n",
      "[209]\ttest-mlogloss:1.981716\ttrain-mlogloss:1.646669\n",
      "[210]\ttest-mlogloss:1.981219\ttrain-mlogloss:1.645733\n",
      "[211]\ttest-mlogloss:1.980680\ttrain-mlogloss:1.643967\n",
      "[212]\ttest-mlogloss:1.979936\ttrain-mlogloss:1.642835\n",
      "[213]\ttest-mlogloss:1.979481\ttrain-mlogloss:1.641210\n",
      "[214]\ttest-mlogloss:1.978828\ttrain-mlogloss:1.638980\n",
      "[215]\ttest-mlogloss:1.978215\ttrain-mlogloss:1.637051\n",
      "[216]\ttest-mlogloss:1.977780\ttrain-mlogloss:1.635838\n",
      "[217]\ttest-mlogloss:1.977154\ttrain-mlogloss:1.634026\n",
      "[218]\ttest-mlogloss:1.976806\ttrain-mlogloss:1.632615\n",
      "[219]\ttest-mlogloss:1.976207\ttrain-mlogloss:1.631776\n",
      "[220]\ttest-mlogloss:1.975783\ttrain-mlogloss:1.630963\n",
      "[221]\ttest-mlogloss:1.975298\ttrain-mlogloss:1.628957\n",
      "[222]\ttest-mlogloss:1.974790\ttrain-mlogloss:1.626913\n",
      "[223]\ttest-mlogloss:1.974355\ttrain-mlogloss:1.625252\n",
      "[224]\ttest-mlogloss:1.974046\ttrain-mlogloss:1.623396\n",
      "[225]\ttest-mlogloss:1.973637\ttrain-mlogloss:1.621871\n",
      "[226]\ttest-mlogloss:1.973080\ttrain-mlogloss:1.619805\n",
      "[227]\ttest-mlogloss:1.972353\ttrain-mlogloss:1.617961\n",
      "[228]\ttest-mlogloss:1.971968\ttrain-mlogloss:1.616282\n",
      "[229]\ttest-mlogloss:1.971428\ttrain-mlogloss:1.614491\n",
      "[230]\ttest-mlogloss:1.970767\ttrain-mlogloss:1.612802\n",
      "[231]\ttest-mlogloss:1.970348\ttrain-mlogloss:1.611606\n",
      "[232]\ttest-mlogloss:1.969694\ttrain-mlogloss:1.610364\n",
      "[233]\ttest-mlogloss:1.969341\ttrain-mlogloss:1.609316\n",
      "[234]\ttest-mlogloss:1.968435\ttrain-mlogloss:1.607726\n",
      "[235]\ttest-mlogloss:1.967801\ttrain-mlogloss:1.606624\n",
      "[236]\ttest-mlogloss:1.967171\ttrain-mlogloss:1.605331\n",
      "[237]\ttest-mlogloss:1.966723\ttrain-mlogloss:1.603771\n",
      "[238]\ttest-mlogloss:1.966331\ttrain-mlogloss:1.602992\n",
      "[239]\ttest-mlogloss:1.965889\ttrain-mlogloss:1.601842\n",
      "[240]\ttest-mlogloss:1.965440\ttrain-mlogloss:1.599847\n",
      "[241]\ttest-mlogloss:1.964869\ttrain-mlogloss:1.597743\n",
      "[242]\ttest-mlogloss:1.964714\ttrain-mlogloss:1.595736\n",
      "[243]\ttest-mlogloss:1.964191\ttrain-mlogloss:1.594568\n",
      "[244]\ttest-mlogloss:1.963452\ttrain-mlogloss:1.593320\n",
      "[245]\ttest-mlogloss:1.962892\ttrain-mlogloss:1.592304\n",
      "[246]\ttest-mlogloss:1.962373\ttrain-mlogloss:1.590867\n",
      "[247]\ttest-mlogloss:1.962100\ttrain-mlogloss:1.589016\n",
      "[248]\ttest-mlogloss:1.961648\ttrain-mlogloss:1.587813\n",
      "[249]\ttest-mlogloss:1.961142\ttrain-mlogloss:1.586564\n",
      "[250]\ttest-mlogloss:1.960801\ttrain-mlogloss:1.585048\n",
      "[251]\ttest-mlogloss:1.960385\ttrain-mlogloss:1.583685\n",
      "[252]\ttest-mlogloss:1.959960\ttrain-mlogloss:1.582850\n",
      "[253]\ttest-mlogloss:1.959507\ttrain-mlogloss:1.581975\n",
      "[254]\ttest-mlogloss:1.958953\ttrain-mlogloss:1.580092\n",
      "[255]\ttest-mlogloss:1.958486\ttrain-mlogloss:1.578618\n",
      "[256]\ttest-mlogloss:1.957750\ttrain-mlogloss:1.577785\n",
      "[257]\ttest-mlogloss:1.957348\ttrain-mlogloss:1.576736\n",
      "[258]\ttest-mlogloss:1.956773\ttrain-mlogloss:1.575514\n",
      "[259]\ttest-mlogloss:1.956510\ttrain-mlogloss:1.574416\n",
      "[260]\ttest-mlogloss:1.956090\ttrain-mlogloss:1.573510\n",
      "[261]\ttest-mlogloss:1.955534\ttrain-mlogloss:1.572365\n",
      "[262]\ttest-mlogloss:1.955133\ttrain-mlogloss:1.570377\n",
      "[263]\ttest-mlogloss:1.954589\ttrain-mlogloss:1.569749\n",
      "[264]\ttest-mlogloss:1.954085\ttrain-mlogloss:1.569126\n",
      "[265]\ttest-mlogloss:1.953477\ttrain-mlogloss:1.567841\n",
      "[266]\ttest-mlogloss:1.953115\ttrain-mlogloss:1.566986\n",
      "[267]\ttest-mlogloss:1.952749\ttrain-mlogloss:1.566009\n",
      "[268]\ttest-mlogloss:1.952456\ttrain-mlogloss:1.565021\n",
      "[269]\ttest-mlogloss:1.951989\ttrain-mlogloss:1.563659\n",
      "[270]\ttest-mlogloss:1.951792\ttrain-mlogloss:1.562359\n",
      "[271]\ttest-mlogloss:1.951290\ttrain-mlogloss:1.560224\n",
      "[272]\ttest-mlogloss:1.950849\ttrain-mlogloss:1.559095\n",
      "[273]\ttest-mlogloss:1.950887\ttrain-mlogloss:1.556890\n",
      "[274]\ttest-mlogloss:1.950576\ttrain-mlogloss:1.555563\n",
      "[275]\ttest-mlogloss:1.950214\ttrain-mlogloss:1.554752\n",
      "[276]\ttest-mlogloss:1.949809\ttrain-mlogloss:1.553671\n",
      "[277]\ttest-mlogloss:1.949292\ttrain-mlogloss:1.552710\n",
      "[278]\ttest-mlogloss:1.948804\ttrain-mlogloss:1.552255\n",
      "[279]\ttest-mlogloss:1.948462\ttrain-mlogloss:1.551248\n",
      "[280]\ttest-mlogloss:1.948054\ttrain-mlogloss:1.550476\n",
      "[281]\ttest-mlogloss:1.947766\ttrain-mlogloss:1.549301\n",
      "[282]\ttest-mlogloss:1.947496\ttrain-mlogloss:1.548345\n",
      "[283]\ttest-mlogloss:1.947029\ttrain-mlogloss:1.546901\n",
      "[284]\ttest-mlogloss:1.946747\ttrain-mlogloss:1.545673\n",
      "[285]\ttest-mlogloss:1.946176\ttrain-mlogloss:1.544448\n",
      "[286]\ttest-mlogloss:1.945479\ttrain-mlogloss:1.542433\n",
      "[287]\ttest-mlogloss:1.945039\ttrain-mlogloss:1.541624\n",
      "[288]\ttest-mlogloss:1.944656\ttrain-mlogloss:1.540676\n",
      "[289]\ttest-mlogloss:1.944387\ttrain-mlogloss:1.539040\n",
      "[290]\ttest-mlogloss:1.943997\ttrain-mlogloss:1.537631\n",
      "[291]\ttest-mlogloss:1.943700\ttrain-mlogloss:1.536242\n",
      "[292]\ttest-mlogloss:1.943263\ttrain-mlogloss:1.535455\n",
      "[293]\ttest-mlogloss:1.942991\ttrain-mlogloss:1.534424\n",
      "[294]\ttest-mlogloss:1.942727\ttrain-mlogloss:1.533097\n",
      "[295]\ttest-mlogloss:1.942384\ttrain-mlogloss:1.532104\n",
      "[296]\ttest-mlogloss:1.941722\ttrain-mlogloss:1.530664\n",
      "[297]\ttest-mlogloss:1.941299\ttrain-mlogloss:1.529861\n",
      "[298]\ttest-mlogloss:1.941009\ttrain-mlogloss:1.529308\n",
      "[299]\ttest-mlogloss:1.940768\ttrain-mlogloss:1.528347\n",
      "[300]\ttest-mlogloss:1.940323\ttrain-mlogloss:1.527849\n",
      "[301]\ttest-mlogloss:1.939956\ttrain-mlogloss:1.527090\n",
      "[302]\ttest-mlogloss:1.939620\ttrain-mlogloss:1.526643\n",
      "[303]\ttest-mlogloss:1.939353\ttrain-mlogloss:1.525897\n",
      "[304]\ttest-mlogloss:1.939061\ttrain-mlogloss:1.524832\n",
      "[305]\ttest-mlogloss:1.938894\ttrain-mlogloss:1.524295\n",
      "[306]\ttest-mlogloss:1.938615\ttrain-mlogloss:1.523649\n",
      "[307]\ttest-mlogloss:1.938304\ttrain-mlogloss:1.522841\n",
      "[308]\ttest-mlogloss:1.938140\ttrain-mlogloss:1.521416\n",
      "[309]\ttest-mlogloss:1.937714\ttrain-mlogloss:1.519887\n",
      "[310]\ttest-mlogloss:1.937477\ttrain-mlogloss:1.518756\n",
      "[311]\ttest-mlogloss:1.937148\ttrain-mlogloss:1.517609\n",
      "[312]\ttest-mlogloss:1.936971\ttrain-mlogloss:1.516612\n",
      "[313]\ttest-mlogloss:1.936359\ttrain-mlogloss:1.515149\n",
      "[314]\ttest-mlogloss:1.935922\ttrain-mlogloss:1.513834\n",
      "[315]\ttest-mlogloss:1.935633\ttrain-mlogloss:1.512280\n",
      "[316]\ttest-mlogloss:1.935076\ttrain-mlogloss:1.510435\n",
      "[317]\ttest-mlogloss:1.934661\ttrain-mlogloss:1.508823\n",
      "[318]\ttest-mlogloss:1.933973\ttrain-mlogloss:1.507576\n",
      "[319]\ttest-mlogloss:1.933518\ttrain-mlogloss:1.506972\n",
      "[320]\ttest-mlogloss:1.933061\ttrain-mlogloss:1.505091\n",
      "[321]\ttest-mlogloss:1.932459\ttrain-mlogloss:1.502600\n",
      "[322]\ttest-mlogloss:1.932109\ttrain-mlogloss:1.501515\n",
      "[323]\ttest-mlogloss:1.931433\ttrain-mlogloss:1.500073\n",
      "[324]\ttest-mlogloss:1.931230\ttrain-mlogloss:1.499192\n",
      "[325]\ttest-mlogloss:1.930960\ttrain-mlogloss:1.497776\n",
      "[326]\ttest-mlogloss:1.930603\ttrain-mlogloss:1.496463\n",
      "[327]\ttest-mlogloss:1.930154\ttrain-mlogloss:1.495335\n",
      "[328]\ttest-mlogloss:1.929867\ttrain-mlogloss:1.494110\n",
      "[329]\ttest-mlogloss:1.929500\ttrain-mlogloss:1.492324\n",
      "[330]\ttest-mlogloss:1.929110\ttrain-mlogloss:1.491064\n",
      "[331]\ttest-mlogloss:1.928658\ttrain-mlogloss:1.489980\n",
      "[332]\ttest-mlogloss:1.928072\ttrain-mlogloss:1.488092\n",
      "[333]\ttest-mlogloss:1.927813\ttrain-mlogloss:1.487186\n",
      "[334]\ttest-mlogloss:1.927644\ttrain-mlogloss:1.486511\n",
      "[335]\ttest-mlogloss:1.927348\ttrain-mlogloss:1.485398\n",
      "[336]\ttest-mlogloss:1.927065\ttrain-mlogloss:1.484199\n",
      "[337]\ttest-mlogloss:1.926604\ttrain-mlogloss:1.482314\n",
      "[338]\ttest-mlogloss:1.926161\ttrain-mlogloss:1.480558\n",
      "[339]\ttest-mlogloss:1.925725\ttrain-mlogloss:1.479366\n",
      "[340]\ttest-mlogloss:1.925348\ttrain-mlogloss:1.478772\n",
      "[341]\ttest-mlogloss:1.924950\ttrain-mlogloss:1.477568\n",
      "[342]\ttest-mlogloss:1.924644\ttrain-mlogloss:1.476503\n",
      "[343]\ttest-mlogloss:1.924352\ttrain-mlogloss:1.475378\n",
      "[344]\ttest-mlogloss:1.924111\ttrain-mlogloss:1.474413\n",
      "[345]\ttest-mlogloss:1.923704\ttrain-mlogloss:1.473805\n",
      "[346]\ttest-mlogloss:1.923335\ttrain-mlogloss:1.473014\n",
      "[347]\ttest-mlogloss:1.922785\ttrain-mlogloss:1.471813\n",
      "[348]\ttest-mlogloss:1.922509\ttrain-mlogloss:1.471252\n",
      "[349]\ttest-mlogloss:1.922358\ttrain-mlogloss:1.470578\n",
      "[350]\ttest-mlogloss:1.921956\ttrain-mlogloss:1.469833\n",
      "[351]\ttest-mlogloss:1.921733\ttrain-mlogloss:1.469027\n",
      "[352]\ttest-mlogloss:1.921508\ttrain-mlogloss:1.468479\n",
      "[353]\ttest-mlogloss:1.921231\ttrain-mlogloss:1.467539\n",
      "[354]\ttest-mlogloss:1.920887\ttrain-mlogloss:1.466716\n",
      "[355]\ttest-mlogloss:1.920505\ttrain-mlogloss:1.465489\n",
      "[356]\ttest-mlogloss:1.920082\ttrain-mlogloss:1.464186\n",
      "[357]\ttest-mlogloss:1.919578\ttrain-mlogloss:1.462741\n",
      "[358]\ttest-mlogloss:1.919200\ttrain-mlogloss:1.461750\n",
      "[359]\ttest-mlogloss:1.918773\ttrain-mlogloss:1.461211\n",
      "[360]\ttest-mlogloss:1.918241\ttrain-mlogloss:1.459992\n",
      "[361]\ttest-mlogloss:1.917790\ttrain-mlogloss:1.458814\n",
      "[362]\ttest-mlogloss:1.917497\ttrain-mlogloss:1.458170\n",
      "[363]\ttest-mlogloss:1.917307\ttrain-mlogloss:1.457643\n",
      "[364]\ttest-mlogloss:1.917134\ttrain-mlogloss:1.456692\n",
      "[365]\ttest-mlogloss:1.916779\ttrain-mlogloss:1.455552\n",
      "[366]\ttest-mlogloss:1.916496\ttrain-mlogloss:1.454309\n",
      "[367]\ttest-mlogloss:1.916154\ttrain-mlogloss:1.453444\n",
      "[368]\ttest-mlogloss:1.915641\ttrain-mlogloss:1.452267\n",
      "[369]\ttest-mlogloss:1.915425\ttrain-mlogloss:1.451225\n",
      "[370]\ttest-mlogloss:1.914951\ttrain-mlogloss:1.449805\n",
      "[371]\ttest-mlogloss:1.914609\ttrain-mlogloss:1.448833\n",
      "[372]\ttest-mlogloss:1.914311\ttrain-mlogloss:1.447310\n",
      "[373]\ttest-mlogloss:1.914079\ttrain-mlogloss:1.446290\n",
      "[374]\ttest-mlogloss:1.913777\ttrain-mlogloss:1.445154\n",
      "[375]\ttest-mlogloss:1.913441\ttrain-mlogloss:1.444473\n",
      "[376]\ttest-mlogloss:1.913143\ttrain-mlogloss:1.443733\n",
      "[377]\ttest-mlogloss:1.912731\ttrain-mlogloss:1.442355\n",
      "[378]\ttest-mlogloss:1.912470\ttrain-mlogloss:1.440620\n",
      "[379]\ttest-mlogloss:1.912070\ttrain-mlogloss:1.439657\n",
      "[380]\ttest-mlogloss:1.911734\ttrain-mlogloss:1.438273\n",
      "[381]\ttest-mlogloss:1.911424\ttrain-mlogloss:1.436243\n",
      "[382]\ttest-mlogloss:1.910914\ttrain-mlogloss:1.434789\n",
      "[383]\ttest-mlogloss:1.910499\ttrain-mlogloss:1.434022\n",
      "[384]\ttest-mlogloss:1.910435\ttrain-mlogloss:1.433094\n",
      "[385]\ttest-mlogloss:1.910165\ttrain-mlogloss:1.432117\n",
      "[386]\ttest-mlogloss:1.909756\ttrain-mlogloss:1.430924\n",
      "[387]\ttest-mlogloss:1.909501\ttrain-mlogloss:1.430098\n",
      "[388]\ttest-mlogloss:1.909306\ttrain-mlogloss:1.428968\n",
      "[389]\ttest-mlogloss:1.909073\ttrain-mlogloss:1.427691\n",
      "[390]\ttest-mlogloss:1.908888\ttrain-mlogloss:1.426865\n",
      "[391]\ttest-mlogloss:1.908509\ttrain-mlogloss:1.426015\n",
      "[392]\ttest-mlogloss:1.908101\ttrain-mlogloss:1.425263\n",
      "[393]\ttest-mlogloss:1.907692\ttrain-mlogloss:1.424465\n",
      "[394]\ttest-mlogloss:1.907374\ttrain-mlogloss:1.423579\n",
      "[395]\ttest-mlogloss:1.907114\ttrain-mlogloss:1.422689\n",
      "[396]\ttest-mlogloss:1.906821\ttrain-mlogloss:1.421714\n",
      "[397]\ttest-mlogloss:1.906630\ttrain-mlogloss:1.421128\n",
      "[398]\ttest-mlogloss:1.906403\ttrain-mlogloss:1.420057\n",
      "[399]\ttest-mlogloss:1.906061\ttrain-mlogloss:1.419234\n",
      "[400]\ttest-mlogloss:1.905586\ttrain-mlogloss:1.417681\n",
      "[401]\ttest-mlogloss:1.905165\ttrain-mlogloss:1.416321\n",
      "[402]\ttest-mlogloss:1.904686\ttrain-mlogloss:1.415461\n",
      "[403]\ttest-mlogloss:1.904419\ttrain-mlogloss:1.414618\n",
      "[404]\ttest-mlogloss:1.904205\ttrain-mlogloss:1.414032\n",
      "[405]\ttest-mlogloss:1.904040\ttrain-mlogloss:1.413069\n",
      "[406]\ttest-mlogloss:1.903581\ttrain-mlogloss:1.411970\n",
      "[407]\ttest-mlogloss:1.903351\ttrain-mlogloss:1.410494\n",
      "[408]\ttest-mlogloss:1.902830\ttrain-mlogloss:1.407925\n",
      "[409]\ttest-mlogloss:1.902572\ttrain-mlogloss:1.406592\n",
      "[410]\ttest-mlogloss:1.902145\ttrain-mlogloss:1.404756\n",
      "[411]\ttest-mlogloss:1.901631\ttrain-mlogloss:1.403524\n",
      "[412]\ttest-mlogloss:1.901519\ttrain-mlogloss:1.402668\n",
      "[413]\ttest-mlogloss:1.901245\ttrain-mlogloss:1.401649\n",
      "[414]\ttest-mlogloss:1.900714\ttrain-mlogloss:1.400875\n",
      "[415]\ttest-mlogloss:1.900401\ttrain-mlogloss:1.400092\n",
      "[416]\ttest-mlogloss:1.900051\ttrain-mlogloss:1.399464\n",
      "[417]\ttest-mlogloss:1.899559\ttrain-mlogloss:1.398676\n",
      "[418]\ttest-mlogloss:1.899347\ttrain-mlogloss:1.397709\n",
      "[419]\ttest-mlogloss:1.899088\ttrain-mlogloss:1.396876\n",
      "[420]\ttest-mlogloss:1.898789\ttrain-mlogloss:1.396514\n",
      "[421]\ttest-mlogloss:1.898636\ttrain-mlogloss:1.395560\n",
      "[422]\ttest-mlogloss:1.898250\ttrain-mlogloss:1.394579\n",
      "[423]\ttest-mlogloss:1.898008\ttrain-mlogloss:1.393554\n",
      "[424]\ttest-mlogloss:1.897906\ttrain-mlogloss:1.392644\n",
      "[425]\ttest-mlogloss:1.897550\ttrain-mlogloss:1.391919\n",
      "[426]\ttest-mlogloss:1.897443\ttrain-mlogloss:1.391302\n",
      "[427]\ttest-mlogloss:1.897347\ttrain-mlogloss:1.390621\n",
      "[428]\ttest-mlogloss:1.896994\ttrain-mlogloss:1.389693\n",
      "[429]\ttest-mlogloss:1.896752\ttrain-mlogloss:1.389260\n",
      "[430]\ttest-mlogloss:1.896585\ttrain-mlogloss:1.388777\n",
      "[431]\ttest-mlogloss:1.896324\ttrain-mlogloss:1.388364\n",
      "[432]\ttest-mlogloss:1.896042\ttrain-mlogloss:1.387551\n",
      "[433]\ttest-mlogloss:1.895789\ttrain-mlogloss:1.386765\n",
      "[434]\ttest-mlogloss:1.895635\ttrain-mlogloss:1.386322\n",
      "[435]\ttest-mlogloss:1.895408\ttrain-mlogloss:1.385718\n",
      "[436]\ttest-mlogloss:1.895119\ttrain-mlogloss:1.385350\n",
      "[437]\ttest-mlogloss:1.894946\ttrain-mlogloss:1.384000\n",
      "[438]\ttest-mlogloss:1.894394\ttrain-mlogloss:1.382313\n",
      "[439]\ttest-mlogloss:1.894035\ttrain-mlogloss:1.380854\n",
      "[440]\ttest-mlogloss:1.893821\ttrain-mlogloss:1.379916\n",
      "[441]\ttest-mlogloss:1.893448\ttrain-mlogloss:1.378783\n",
      "[442]\ttest-mlogloss:1.893029\ttrain-mlogloss:1.378138\n",
      "[443]\ttest-mlogloss:1.892749\ttrain-mlogloss:1.377041\n",
      "[444]\ttest-mlogloss:1.892132\ttrain-mlogloss:1.375537\n",
      "[445]\ttest-mlogloss:1.891966\ttrain-mlogloss:1.374933\n",
      "[446]\ttest-mlogloss:1.891835\ttrain-mlogloss:1.374075\n",
      "[447]\ttest-mlogloss:1.891633\ttrain-mlogloss:1.372997\n",
      "[448]\ttest-mlogloss:1.891352\ttrain-mlogloss:1.371997\n",
      "[449]\ttest-mlogloss:1.891111\ttrain-mlogloss:1.371659\n",
      "[450]\ttest-mlogloss:1.890952\ttrain-mlogloss:1.371234\n",
      "[451]\ttest-mlogloss:1.890544\ttrain-mlogloss:1.370667\n",
      "[452]\ttest-mlogloss:1.890221\ttrain-mlogloss:1.370312\n",
      "[453]\ttest-mlogloss:1.889941\ttrain-mlogloss:1.369976\n",
      "[454]\ttest-mlogloss:1.889711\ttrain-mlogloss:1.369316\n",
      "[455]\ttest-mlogloss:1.889557\ttrain-mlogloss:1.368703\n",
      "[456]\ttest-mlogloss:1.889298\ttrain-mlogloss:1.367839\n",
      "[457]\ttest-mlogloss:1.888833\ttrain-mlogloss:1.366451\n",
      "[458]\ttest-mlogloss:1.888685\ttrain-mlogloss:1.365760\n",
      "[459]\ttest-mlogloss:1.888575\ttrain-mlogloss:1.365295\n",
      "[460]\ttest-mlogloss:1.888314\ttrain-mlogloss:1.363976\n",
      "[461]\ttest-mlogloss:1.887928\ttrain-mlogloss:1.363157\n",
      "[462]\ttest-mlogloss:1.887626\ttrain-mlogloss:1.362804\n",
      "[463]\ttest-mlogloss:1.887427\ttrain-mlogloss:1.362242\n",
      "[464]\ttest-mlogloss:1.887094\ttrain-mlogloss:1.361580\n",
      "[465]\ttest-mlogloss:1.886874\ttrain-mlogloss:1.360805\n",
      "[466]\ttest-mlogloss:1.886567\ttrain-mlogloss:1.360117\n",
      "[467]\ttest-mlogloss:1.886294\ttrain-mlogloss:1.359526\n",
      "[468]\ttest-mlogloss:1.886160\ttrain-mlogloss:1.358937\n",
      "[469]\ttest-mlogloss:1.885939\ttrain-mlogloss:1.358380\n",
      "[470]\ttest-mlogloss:1.885707\ttrain-mlogloss:1.357721\n",
      "[471]\ttest-mlogloss:1.885431\ttrain-mlogloss:1.357220\n",
      "[472]\ttest-mlogloss:1.885286\ttrain-mlogloss:1.356683\n",
      "[473]\ttest-mlogloss:1.884860\ttrain-mlogloss:1.355542\n",
      "[474]\ttest-mlogloss:1.884627\ttrain-mlogloss:1.354720\n",
      "[475]\ttest-mlogloss:1.884251\ttrain-mlogloss:1.354042\n",
      "[476]\ttest-mlogloss:1.883948\ttrain-mlogloss:1.353285\n",
      "[477]\ttest-mlogloss:1.883530\ttrain-mlogloss:1.352344\n",
      "[478]\ttest-mlogloss:1.883318\ttrain-mlogloss:1.351919\n",
      "[479]\ttest-mlogloss:1.883080\ttrain-mlogloss:1.351426\n",
      "[480]\ttest-mlogloss:1.882724\ttrain-mlogloss:1.350232\n",
      "[481]\ttest-mlogloss:1.882445\ttrain-mlogloss:1.349331\n",
      "[482]\ttest-mlogloss:1.882241\ttrain-mlogloss:1.348563\n",
      "[483]\ttest-mlogloss:1.881943\ttrain-mlogloss:1.347525\n",
      "[484]\ttest-mlogloss:1.881674\ttrain-mlogloss:1.346883\n",
      "[485]\ttest-mlogloss:1.881603\ttrain-mlogloss:1.345907\n",
      "[486]\ttest-mlogloss:1.881314\ttrain-mlogloss:1.345086\n",
      "[487]\ttest-mlogloss:1.881030\ttrain-mlogloss:1.344182\n",
      "[488]\ttest-mlogloss:1.880746\ttrain-mlogloss:1.342954\n",
      "[489]\ttest-mlogloss:1.880570\ttrain-mlogloss:1.341795\n",
      "[490]\ttest-mlogloss:1.880305\ttrain-mlogloss:1.340544\n",
      "[491]\ttest-mlogloss:1.880090\ttrain-mlogloss:1.339883\n",
      "[492]\ttest-mlogloss:1.879831\ttrain-mlogloss:1.339378\n",
      "[493]\ttest-mlogloss:1.879578\ttrain-mlogloss:1.338958\n",
      "[494]\ttest-mlogloss:1.879350\ttrain-mlogloss:1.338672\n",
      "[495]\ttest-mlogloss:1.879144\ttrain-mlogloss:1.337912\n",
      "[496]\ttest-mlogloss:1.878858\ttrain-mlogloss:1.337307\n",
      "[497]\ttest-mlogloss:1.878577\ttrain-mlogloss:1.336278\n",
      "[498]\ttest-mlogloss:1.878478\ttrain-mlogloss:1.335381\n",
      "[499]\ttest-mlogloss:1.878269\ttrain-mlogloss:1.334505\n",
      "[500]\ttest-mlogloss:1.878051\ttrain-mlogloss:1.333978\n",
      "[501]\ttest-mlogloss:1.877646\ttrain-mlogloss:1.332927\n",
      "[502]\ttest-mlogloss:1.877264\ttrain-mlogloss:1.331794\n",
      "[503]\ttest-mlogloss:1.877122\ttrain-mlogloss:1.331364\n",
      "[504]\ttest-mlogloss:1.877006\ttrain-mlogloss:1.330773\n",
      "[505]\ttest-mlogloss:1.876840\ttrain-mlogloss:1.329814\n",
      "[506]\ttest-mlogloss:1.876651\ttrain-mlogloss:1.328949\n",
      "[507]\ttest-mlogloss:1.876387\ttrain-mlogloss:1.327592\n",
      "[508]\ttest-mlogloss:1.875980\ttrain-mlogloss:1.326411\n",
      "[509]\ttest-mlogloss:1.875715\ttrain-mlogloss:1.325834\n",
      "[510]\ttest-mlogloss:1.875435\ttrain-mlogloss:1.325327\n",
      "[511]\ttest-mlogloss:1.875173\ttrain-mlogloss:1.324539\n",
      "[512]\ttest-mlogloss:1.874802\ttrain-mlogloss:1.323713\n",
      "[513]\ttest-mlogloss:1.874515\ttrain-mlogloss:1.322887\n",
      "[514]\ttest-mlogloss:1.874164\ttrain-mlogloss:1.322261\n",
      "[515]\ttest-mlogloss:1.873933\ttrain-mlogloss:1.321668\n",
      "[516]\ttest-mlogloss:1.873803\ttrain-mlogloss:1.320949\n",
      "[517]\ttest-mlogloss:1.873603\ttrain-mlogloss:1.320482\n",
      "[518]\ttest-mlogloss:1.873435\ttrain-mlogloss:1.319491\n",
      "[519]\ttest-mlogloss:1.873163\ttrain-mlogloss:1.319120\n",
      "[520]\ttest-mlogloss:1.872935\ttrain-mlogloss:1.318268\n",
      "[521]\ttest-mlogloss:1.872716\ttrain-mlogloss:1.317983\n",
      "[522]\ttest-mlogloss:1.872530\ttrain-mlogloss:1.317528\n",
      "[523]\ttest-mlogloss:1.872417\ttrain-mlogloss:1.316267\n",
      "[524]\ttest-mlogloss:1.872241\ttrain-mlogloss:1.315480\n",
      "[525]\ttest-mlogloss:1.871925\ttrain-mlogloss:1.315000\n",
      "[526]\ttest-mlogloss:1.871759\ttrain-mlogloss:1.314661\n",
      "[527]\ttest-mlogloss:1.871607\ttrain-mlogloss:1.314085\n",
      "[528]\ttest-mlogloss:1.871360\ttrain-mlogloss:1.313091\n",
      "[529]\ttest-mlogloss:1.871034\ttrain-mlogloss:1.312001\n",
      "[530]\ttest-mlogloss:1.870803\ttrain-mlogloss:1.310945\n",
      "[531]\ttest-mlogloss:1.870660\ttrain-mlogloss:1.309975\n",
      "[532]\ttest-mlogloss:1.870399\ttrain-mlogloss:1.308916\n",
      "[533]\ttest-mlogloss:1.870103\ttrain-mlogloss:1.308197\n",
      "[534]\ttest-mlogloss:1.869839\ttrain-mlogloss:1.307478\n",
      "[535]\ttest-mlogloss:1.869613\ttrain-mlogloss:1.306799\n",
      "[536]\ttest-mlogloss:1.869436\ttrain-mlogloss:1.306175\n",
      "[537]\ttest-mlogloss:1.869136\ttrain-mlogloss:1.305367\n",
      "[538]\ttest-mlogloss:1.868705\ttrain-mlogloss:1.304199\n",
      "[539]\ttest-mlogloss:1.868420\ttrain-mlogloss:1.303477\n",
      "[540]\ttest-mlogloss:1.868256\ttrain-mlogloss:1.302737\n",
      "[541]\ttest-mlogloss:1.868095\ttrain-mlogloss:1.302346\n",
      "[542]\ttest-mlogloss:1.867721\ttrain-mlogloss:1.301764\n",
      "[543]\ttest-mlogloss:1.867443\ttrain-mlogloss:1.301196\n",
      "[544]\ttest-mlogloss:1.867259\ttrain-mlogloss:1.300493\n",
      "[545]\ttest-mlogloss:1.867196\ttrain-mlogloss:1.299786\n",
      "[546]\ttest-mlogloss:1.866914\ttrain-mlogloss:1.299506\n",
      "[547]\ttest-mlogloss:1.866738\ttrain-mlogloss:1.299262\n",
      "[548]\ttest-mlogloss:1.866592\ttrain-mlogloss:1.298818\n",
      "[549]\ttest-mlogloss:1.866449\ttrain-mlogloss:1.298270\n",
      "[550]\ttest-mlogloss:1.866398\ttrain-mlogloss:1.297829\n",
      "[551]\ttest-mlogloss:1.866236\ttrain-mlogloss:1.297463\n",
      "[552]\ttest-mlogloss:1.865993\ttrain-mlogloss:1.296886\n",
      "[553]\ttest-mlogloss:1.865866\ttrain-mlogloss:1.296370\n",
      "[554]\ttest-mlogloss:1.865708\ttrain-mlogloss:1.295681\n",
      "[555]\ttest-mlogloss:1.864997\ttrain-mlogloss:1.294670\n",
      "[556]\ttest-mlogloss:1.864682\ttrain-mlogloss:1.294182\n",
      "[557]\ttest-mlogloss:1.864364\ttrain-mlogloss:1.293535\n",
      "[558]\ttest-mlogloss:1.864214\ttrain-mlogloss:1.293135\n",
      "[559]\ttest-mlogloss:1.863989\ttrain-mlogloss:1.292420\n",
      "[560]\ttest-mlogloss:1.863904\ttrain-mlogloss:1.291692\n",
      "[561]\ttest-mlogloss:1.863557\ttrain-mlogloss:1.291008\n",
      "[562]\ttest-mlogloss:1.863336\ttrain-mlogloss:1.290136\n",
      "[563]\ttest-mlogloss:1.863081\ttrain-mlogloss:1.289630\n",
      "[564]\ttest-mlogloss:1.862995\ttrain-mlogloss:1.289417\n",
      "[565]\ttest-mlogloss:1.862896\ttrain-mlogloss:1.289049\n",
      "[566]\ttest-mlogloss:1.862826\ttrain-mlogloss:1.288782\n",
      "[567]\ttest-mlogloss:1.862729\ttrain-mlogloss:1.288394\n",
      "[568]\ttest-mlogloss:1.862476\ttrain-mlogloss:1.287341\n",
      "[569]\ttest-mlogloss:1.862192\ttrain-mlogloss:1.286617\n",
      "[570]\ttest-mlogloss:1.861965\ttrain-mlogloss:1.285975\n",
      "[571]\ttest-mlogloss:1.861826\ttrain-mlogloss:1.285598\n",
      "[572]\ttest-mlogloss:1.861744\ttrain-mlogloss:1.285187\n",
      "[573]\ttest-mlogloss:1.861631\ttrain-mlogloss:1.284523\n",
      "[574]\ttest-mlogloss:1.861379\ttrain-mlogloss:1.283549\n",
      "[575]\ttest-mlogloss:1.861100\ttrain-mlogloss:1.282908\n",
      "[576]\ttest-mlogloss:1.860766\ttrain-mlogloss:1.281845\n",
      "[577]\ttest-mlogloss:1.860474\ttrain-mlogloss:1.280933\n",
      "[578]\ttest-mlogloss:1.860290\ttrain-mlogloss:1.280087\n",
      "[579]\ttest-mlogloss:1.860129\ttrain-mlogloss:1.279229\n",
      "[580]\ttest-mlogloss:1.859694\ttrain-mlogloss:1.278223\n",
      "[581]\ttest-mlogloss:1.859447\ttrain-mlogloss:1.277660\n",
      "[582]\ttest-mlogloss:1.859262\ttrain-mlogloss:1.276602\n",
      "[583]\ttest-mlogloss:1.858976\ttrain-mlogloss:1.275516\n",
      "[584]\ttest-mlogloss:1.858763\ttrain-mlogloss:1.274636\n",
      "[585]\ttest-mlogloss:1.858493\ttrain-mlogloss:1.273344\n",
      "[586]\ttest-mlogloss:1.858252\ttrain-mlogloss:1.271898\n",
      "[587]\ttest-mlogloss:1.858065\ttrain-mlogloss:1.271214\n",
      "[588]\ttest-mlogloss:1.857707\ttrain-mlogloss:1.269420\n",
      "[589]\ttest-mlogloss:1.857346\ttrain-mlogloss:1.268145\n",
      "[590]\ttest-mlogloss:1.857123\ttrain-mlogloss:1.266959\n",
      "[591]\ttest-mlogloss:1.856953\ttrain-mlogloss:1.266091\n",
      "[592]\ttest-mlogloss:1.856708\ttrain-mlogloss:1.265407\n",
      "[593]\ttest-mlogloss:1.856492\ttrain-mlogloss:1.264010\n",
      "[594]\ttest-mlogloss:1.856288\ttrain-mlogloss:1.262926\n",
      "[595]\ttest-mlogloss:1.856121\ttrain-mlogloss:1.261456\n",
      "[596]\ttest-mlogloss:1.855772\ttrain-mlogloss:1.259887\n",
      "[597]\ttest-mlogloss:1.855382\ttrain-mlogloss:1.258825\n",
      "[598]\ttest-mlogloss:1.855079\ttrain-mlogloss:1.257637\n",
      "[599]\ttest-mlogloss:1.854808\ttrain-mlogloss:1.256878\n",
      "[600]\ttest-mlogloss:1.854599\ttrain-mlogloss:1.256196\n",
      "[601]\ttest-mlogloss:1.854227\ttrain-mlogloss:1.255674\n",
      "[602]\ttest-mlogloss:1.853804\ttrain-mlogloss:1.254964\n",
      "[603]\ttest-mlogloss:1.853381\ttrain-mlogloss:1.253810\n",
      "[604]\ttest-mlogloss:1.853091\ttrain-mlogloss:1.252530\n",
      "[605]\ttest-mlogloss:1.852877\ttrain-mlogloss:1.251959\n",
      "[606]\ttest-mlogloss:1.852570\ttrain-mlogloss:1.250840\n",
      "[607]\ttest-mlogloss:1.852247\ttrain-mlogloss:1.250020\n",
      "[608]\ttest-mlogloss:1.852060\ttrain-mlogloss:1.249704\n",
      "[609]\ttest-mlogloss:1.851946\ttrain-mlogloss:1.249141\n",
      "[610]\ttest-mlogloss:1.851840\ttrain-mlogloss:1.248350\n",
      "[611]\ttest-mlogloss:1.851674\ttrain-mlogloss:1.247718\n",
      "[612]\ttest-mlogloss:1.851407\ttrain-mlogloss:1.247175\n",
      "[613]\ttest-mlogloss:1.851190\ttrain-mlogloss:1.246559\n",
      "[614]\ttest-mlogloss:1.850947\ttrain-mlogloss:1.245713\n",
      "[615]\ttest-mlogloss:1.850760\ttrain-mlogloss:1.245440\n",
      "[616]\ttest-mlogloss:1.850489\ttrain-mlogloss:1.244485\n",
      "[617]\ttest-mlogloss:1.850332\ttrain-mlogloss:1.244050\n",
      "[618]\ttest-mlogloss:1.850129\ttrain-mlogloss:1.243402\n",
      "[619]\ttest-mlogloss:1.849924\ttrain-mlogloss:1.242969\n",
      "[620]\ttest-mlogloss:1.849748\ttrain-mlogloss:1.242106\n",
      "[621]\ttest-mlogloss:1.849426\ttrain-mlogloss:1.241503\n",
      "[622]\ttest-mlogloss:1.849220\ttrain-mlogloss:1.240668\n",
      "[623]\ttest-mlogloss:1.849054\ttrain-mlogloss:1.240016\n",
      "[624]\ttest-mlogloss:1.848857\ttrain-mlogloss:1.238795\n",
      "[625]\ttest-mlogloss:1.848663\ttrain-mlogloss:1.237743\n",
      "[626]\ttest-mlogloss:1.848674\ttrain-mlogloss:1.237200\n",
      "[627]\ttest-mlogloss:1.848481\ttrain-mlogloss:1.236583\n",
      "[628]\ttest-mlogloss:1.848266\ttrain-mlogloss:1.235902\n",
      "[629]\ttest-mlogloss:1.848045\ttrain-mlogloss:1.235481\n",
      "[630]\ttest-mlogloss:1.847737\ttrain-mlogloss:1.234874\n",
      "[631]\ttest-mlogloss:1.847512\ttrain-mlogloss:1.234442\n",
      "[632]\ttest-mlogloss:1.847279\ttrain-mlogloss:1.233791\n",
      "[633]\ttest-mlogloss:1.847113\ttrain-mlogloss:1.233243\n",
      "[634]\ttest-mlogloss:1.846893\ttrain-mlogloss:1.231928\n",
      "[635]\ttest-mlogloss:1.846668\ttrain-mlogloss:1.231086\n",
      "[636]\ttest-mlogloss:1.846293\ttrain-mlogloss:1.230272\n",
      "[637]\ttest-mlogloss:1.845881\ttrain-mlogloss:1.229352\n",
      "[638]\ttest-mlogloss:1.845798\ttrain-mlogloss:1.228691\n",
      "[639]\ttest-mlogloss:1.845650\ttrain-mlogloss:1.228028\n",
      "[640]\ttest-mlogloss:1.845449\ttrain-mlogloss:1.227456\n",
      "[641]\ttest-mlogloss:1.845234\ttrain-mlogloss:1.226928\n",
      "[642]\ttest-mlogloss:1.845135\ttrain-mlogloss:1.226327\n",
      "[643]\ttest-mlogloss:1.844893\ttrain-mlogloss:1.225460\n",
      "[644]\ttest-mlogloss:1.844570\ttrain-mlogloss:1.224685\n",
      "[645]\ttest-mlogloss:1.844369\ttrain-mlogloss:1.224112\n",
      "[646]\ttest-mlogloss:1.844129\ttrain-mlogloss:1.222985\n",
      "[647]\ttest-mlogloss:1.843968\ttrain-mlogloss:1.222472\n",
      "[648]\ttest-mlogloss:1.843874\ttrain-mlogloss:1.222014\n",
      "[649]\ttest-mlogloss:1.843707\ttrain-mlogloss:1.221059\n",
      "[650]\ttest-mlogloss:1.843430\ttrain-mlogloss:1.220567\n",
      "[651]\ttest-mlogloss:1.843254\ttrain-mlogloss:1.220034\n",
      "[652]\ttest-mlogloss:1.843085\ttrain-mlogloss:1.219849\n",
      "[653]\ttest-mlogloss:1.842931\ttrain-mlogloss:1.219520\n",
      "[654]\ttest-mlogloss:1.842736\ttrain-mlogloss:1.219193\n",
      "[655]\ttest-mlogloss:1.842652\ttrain-mlogloss:1.218584\n",
      "[656]\ttest-mlogloss:1.842455\ttrain-mlogloss:1.217996\n",
      "[657]\ttest-mlogloss:1.842399\ttrain-mlogloss:1.217526\n",
      "[658]\ttest-mlogloss:1.842139\ttrain-mlogloss:1.217032\n",
      "[659]\ttest-mlogloss:1.842047\ttrain-mlogloss:1.216725\n",
      "[660]\ttest-mlogloss:1.841934\ttrain-mlogloss:1.216269\n",
      "[661]\ttest-mlogloss:1.841740\ttrain-mlogloss:1.215763\n",
      "[662]\ttest-mlogloss:1.841402\ttrain-mlogloss:1.214840\n",
      "[663]\ttest-mlogloss:1.841285\ttrain-mlogloss:1.213626\n",
      "[664]\ttest-mlogloss:1.841004\ttrain-mlogloss:1.212577\n",
      "[665]\ttest-mlogloss:1.840763\ttrain-mlogloss:1.211926\n",
      "[666]\ttest-mlogloss:1.840536\ttrain-mlogloss:1.210973\n",
      "[667]\ttest-mlogloss:1.840355\ttrain-mlogloss:1.210032\n",
      "[668]\ttest-mlogloss:1.840067\ttrain-mlogloss:1.209304\n",
      "[669]\ttest-mlogloss:1.839904\ttrain-mlogloss:1.208627\n",
      "[670]\ttest-mlogloss:1.839821\ttrain-mlogloss:1.207462\n",
      "[671]\ttest-mlogloss:1.839523\ttrain-mlogloss:1.206682\n",
      "[672]\ttest-mlogloss:1.839344\ttrain-mlogloss:1.206017\n",
      "[673]\ttest-mlogloss:1.839211\ttrain-mlogloss:1.205679\n",
      "[674]\ttest-mlogloss:1.839060\ttrain-mlogloss:1.204914\n",
      "[675]\ttest-mlogloss:1.838981\ttrain-mlogloss:1.204210\n",
      "[676]\ttest-mlogloss:1.838833\ttrain-mlogloss:1.203655\n",
      "[677]\ttest-mlogloss:1.838691\ttrain-mlogloss:1.203206\n",
      "[678]\ttest-mlogloss:1.838501\ttrain-mlogloss:1.202637\n",
      "[679]\ttest-mlogloss:1.838059\ttrain-mlogloss:1.202012\n",
      "[680]\ttest-mlogloss:1.837908\ttrain-mlogloss:1.201258\n",
      "[681]\ttest-mlogloss:1.837685\ttrain-mlogloss:1.200737\n",
      "[682]\ttest-mlogloss:1.837366\ttrain-mlogloss:1.200126\n",
      "[683]\ttest-mlogloss:1.837303\ttrain-mlogloss:1.199694\n",
      "[684]\ttest-mlogloss:1.837133\ttrain-mlogloss:1.199203\n",
      "[685]\ttest-mlogloss:1.836962\ttrain-mlogloss:1.198656\n",
      "[686]\ttest-mlogloss:1.836684\ttrain-mlogloss:1.197540\n",
      "[687]\ttest-mlogloss:1.836429\ttrain-mlogloss:1.196687\n",
      "[688]\ttest-mlogloss:1.836188\ttrain-mlogloss:1.195984\n",
      "[689]\ttest-mlogloss:1.836112\ttrain-mlogloss:1.195546\n",
      "[690]\ttest-mlogloss:1.835827\ttrain-mlogloss:1.195039\n",
      "[691]\ttest-mlogloss:1.835518\ttrain-mlogloss:1.194530\n",
      "[692]\ttest-mlogloss:1.835371\ttrain-mlogloss:1.194197\n",
      "[693]\ttest-mlogloss:1.835309\ttrain-mlogloss:1.193951\n",
      "[694]\ttest-mlogloss:1.835226\ttrain-mlogloss:1.193671\n",
      "[695]\ttest-mlogloss:1.835120\ttrain-mlogloss:1.193441\n",
      "[696]\ttest-mlogloss:1.835028\ttrain-mlogloss:1.192804\n",
      "[697]\ttest-mlogloss:1.834930\ttrain-mlogloss:1.192369\n",
      "[698]\ttest-mlogloss:1.834791\ttrain-mlogloss:1.192007\n",
      "[699]\ttest-mlogloss:1.834781\ttrain-mlogloss:1.191785\n",
      "[700]\ttest-mlogloss:1.834689\ttrain-mlogloss:1.191041\n",
      "[701]\ttest-mlogloss:1.834545\ttrain-mlogloss:1.190382\n",
      "[702]\ttest-mlogloss:1.834270\ttrain-mlogloss:1.189696\n",
      "[703]\ttest-mlogloss:1.834106\ttrain-mlogloss:1.188743\n",
      "[704]\ttest-mlogloss:1.833966\ttrain-mlogloss:1.188267\n",
      "[705]\ttest-mlogloss:1.833763\ttrain-mlogloss:1.187939\n",
      "[706]\ttest-mlogloss:1.833701\ttrain-mlogloss:1.187690\n",
      "[707]\ttest-mlogloss:1.833437\ttrain-mlogloss:1.186878\n",
      "[708]\ttest-mlogloss:1.833215\ttrain-mlogloss:1.186178\n",
      "[709]\ttest-mlogloss:1.833061\ttrain-mlogloss:1.185494\n",
      "[710]\ttest-mlogloss:1.832936\ttrain-mlogloss:1.184338\n",
      "[711]\ttest-mlogloss:1.832798\ttrain-mlogloss:1.183553\n",
      "[712]\ttest-mlogloss:1.832532\ttrain-mlogloss:1.182832\n",
      "[713]\ttest-mlogloss:1.832290\ttrain-mlogloss:1.182340\n",
      "[714]\ttest-mlogloss:1.832093\ttrain-mlogloss:1.181785\n",
      "[715]\ttest-mlogloss:1.831853\ttrain-mlogloss:1.181298\n",
      "[716]\ttest-mlogloss:1.831714\ttrain-mlogloss:1.180387\n",
      "[717]\ttest-mlogloss:1.831571\ttrain-mlogloss:1.179887\n",
      "[718]\ttest-mlogloss:1.831495\ttrain-mlogloss:1.179533\n",
      "[719]\ttest-mlogloss:1.831333\ttrain-mlogloss:1.179114\n",
      "[720]\ttest-mlogloss:1.831179\ttrain-mlogloss:1.178608\n",
      "[721]\ttest-mlogloss:1.831023\ttrain-mlogloss:1.178236\n",
      "[722]\ttest-mlogloss:1.830891\ttrain-mlogloss:1.177773\n",
      "[723]\ttest-mlogloss:1.830906\ttrain-mlogloss:1.177267\n",
      "[724]\ttest-mlogloss:1.830719\ttrain-mlogloss:1.176207\n",
      "[725]\ttest-mlogloss:1.830503\ttrain-mlogloss:1.175227\n",
      "[726]\ttest-mlogloss:1.830367\ttrain-mlogloss:1.174706\n",
      "[727]\ttest-mlogloss:1.830205\ttrain-mlogloss:1.173531\n",
      "[728]\ttest-mlogloss:1.829971\ttrain-mlogloss:1.173093\n",
      "[729]\ttest-mlogloss:1.829860\ttrain-mlogloss:1.172756\n",
      "[730]\ttest-mlogloss:1.829653\ttrain-mlogloss:1.172454\n",
      "[731]\ttest-mlogloss:1.829590\ttrain-mlogloss:1.172224\n",
      "[732]\ttest-mlogloss:1.829369\ttrain-mlogloss:1.171739\n",
      "[733]\ttest-mlogloss:1.829193\ttrain-mlogloss:1.171383\n",
      "[734]\ttest-mlogloss:1.829116\ttrain-mlogloss:1.170943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[735]\ttest-mlogloss:1.829002\ttrain-mlogloss:1.170495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-39416b34b48c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     gbm = xgb.train(params, dtrain, num_boost_round=5000, \n\u001b[0;32m     21\u001b[0m                     \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                    early_stopping_rounds=10)\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mmean_ndcg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mnboost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mbst_eval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst_eval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTRING_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/miniconda/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36meval_set\u001b[1;34m(self, evals, iteration, feval)\u001b[0m\n\u001b[0;32m    753\u001b[0m             _check_call(_LIB.XGBoosterEvalOneIter(self.handle, iteration,\n\u001b[0;32m    754\u001b[0m                                                   \u001b[0mdmats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m                                                   ctypes.byref(msg)))\n\u001b[0m\u001b[0;32m    756\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 12,\n",
    "    'bst:eta' :  0.3,\n",
    "    'bst:max_depth': 4,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'silent': 0,\n",
    "    'nthread': 2\n",
    "}\n",
    "\n",
    "values, counts = np.unique(y, return_counts=True)\n",
    "freqs = counts/float(counts.sum())\n",
    "weights = 1/freqs\n",
    "\n",
    "for l in [1.0]:\n",
    "    params['lambda'] = l\n",
    "    print \"Lambda\", l\n",
    "    dtest = xgb.DMatrix(X_test.values, label=y_test)\n",
    "    dtrain = xgb.DMatrix(X_train.values, label=y_train, weight=weights.take(y_train))\n",
    "    gbm = xgb.train(params, dtrain, num_boost_round=5000, \n",
    "                    verbose_eval=True, evals=[(dtest, 'test'), (dtrain, 'train')],\n",
    "                   early_stopping_rounds=10)\n",
    "    gbm.predict_proba = gbm.predict\n",
    "    print mean_ndcg(gbm, xgb.DMatrix(X_test.values), y_test)\n",
    "    prediction = gbm.predict(xgb.DMatrix(X_test.values)).argmax(axis=1)\n",
    "    print classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eta 0.5, max_depth=2, lambda=0.1, test-mlogloss:1.038725\ttrain-mlogloss:1.012042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.73 ndcg on balanced dataset for xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print mean_ndcg(gbm, xgb.DMatrix(X_test.values), y_test)\n",
    "print classification_report(prediction, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# max_depth=3 eta=0.5 num_boost_round= 300\n",
    "```\n",
    "Lambda 1.0\n",
    "0.916503864109\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.00      0.00      0.00        22\n",
    "          1       0.00      0.00      0.00        34\n",
    "          2       0.00      0.00      0.00        45\n",
    "          3       0.01      0.09      0.01        70\n",
    "          4       0.01      0.08      0.01       133\n",
    "          5       0.00      0.03      0.00        80\n",
    "          6       0.01      0.10      0.02        83\n",
    "          7       0.86      0.70      0.77     50306\n",
    "          8       0.00      0.00      0.00        39\n",
    "          9       0.00      0.00      0.00        13\n",
    "         10       0.47      0.50      0.49     19225\n",
    "         11       0.02      0.14      0.03       389\n",
    "\n",
    "avg / total       0.74      0.64      0.68     70439\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': 12,\n",
    "    'bst:eta' :  0.5,\n",
    "    'bst:max_depth': 4,\n",
    "    'eval_metric': 'ndcg@5-',\n",
    "    'silent': 0,\n",
    "    'nthread': 2,\n",
    "    'lambda': 1.0\n",
    "}\n",
    "\n",
    "values, counts = np.unique(y, return_counts=True)\n",
    "freqs = counts/float(counts.sum())\n",
    "weights = 1/freqs\n",
    "\n",
    "clf = xgb.train(params, xgb.DMatrix(session_features_train.values, label=y, \n",
    "                                    weight=weights.take(y)), num_boost_round=5000)\n",
    "clf.predict_proba = clf.predict\n",
    "\n",
    "y_pred = clf.predict_proba(xgb.DMatrix(session_features_test.values))\n",
    "\n",
    "ids = []  #list of ids\n",
    "cts = []  #list of countries\n",
    "id_test = test.id.values\n",
    "for i in range(len(id_test)):\n",
    "    idx = id_test[i]\n",
    "    ids += [idx] * 5\n",
    "    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "sub.to_csv(os.path.join('data', 'sub6.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907452273331\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         9\n",
      "          1       0.00      0.00      0.00        15\n",
      "          2       0.00      0.00      0.00        16\n",
      "          3       0.00      0.00      0.00        36\n",
      "          4       0.00      0.02      0.00        88\n",
      "          5       0.00      0.05      0.00        21\n",
      "          6       0.00      0.04      0.00        28\n",
      "          7       0.85      0.68      0.76     51714\n",
      "          8       0.00      0.00      0.00         7\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       0.43      0.49      0.46     18291\n",
      "         11       0.00      0.07      0.01       212\n",
      "\n",
      "avg / total       0.74      0.62      0.67     70439\n",
      "\n",
      "0.911470609321\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         7\n",
      "          1       0.00      0.00      0.00        12\n",
      "          2       0.00      0.00      0.00        13\n",
      "          3       0.00      0.03      0.00        32\n",
      "          4       0.00      0.03      0.00        70\n",
      "          5       0.00      0.00      0.00        19\n",
      "          6       0.00      0.04      0.00        25\n",
      "          7       0.86      0.68      0.76     51607\n",
      "          8       0.00      0.00      0.00         5\n",
      "          9       0.00      0.00      0.00         1\n",
      "         10       0.44      0.49      0.47     18460\n",
      "         11       0.00      0.08      0.01       188\n",
      "\n",
      "avg / total       0.74      0.63      0.68     70439\n",
      "\n",
      "0.913924484375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         8\n",
      "          1       0.00      0.00      0.00        11\n",
      "          2       0.00      0.00      0.00        13\n",
      "          3       0.00      0.03      0.00        34\n",
      "          4       0.00      0.02      0.00        60\n",
      "          5       0.00      0.00      0.00        21\n",
      "          6       0.00      0.04      0.00        28\n",
      "          7       0.86      0.68      0.76     51668\n",
      "          8       0.00      0.00      0.00         3\n",
      "          9       0.00      0.00      0.00         2\n",
      "         10       0.44      0.50      0.47     18426\n",
      "         11       0.00      0.06      0.01       165\n",
      "\n",
      "avg / total       0.75      0.63      0.68     70439\n",
      "\n",
      "0.915370588311\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         7\n",
      "          1       0.00      0.00      0.00         8\n",
      "          2       0.00      0.00      0.00        14\n",
      "          3       0.00      0.03      0.00        29\n",
      "          4       0.00      0.00      0.00        50\n",
      "          5       0.00      0.00      0.00        18\n",
      "          6       0.00      0.04      0.00        27\n",
      "          7       0.86      0.68      0.76     51651\n",
      "          8       0.00      0.00      0.00         4\n",
      "          9       0.00      0.00      0.00         3\n",
      "         10       0.45      0.50      0.47     18482\n",
      "         11       0.00      0.06      0.01       146\n",
      "\n",
      "avg / total       0.75      0.63      0.68     70439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X = final[:train.shape[0]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "n_trees = [20, 40, 80, 160]\n",
    "df = pd.DataFrame(np.nan, index=n_trees, columns=['train_score', 'test_score', 'ndcg_score'])\n",
    "for n in n_trees:\n",
    "    clf = RandomForestClassifier(n_estimators=n, oob_score=True, n_jobs=1, random_state=42, criterion='entropy', max_depth=32)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    \n",
    "    # The RF will give you unusually high training error if calculated this way\n",
    "    # http://stats.stackexchange.com/questions/66543/random-forest-is-overfitting\n",
    "    \n",
    "    # Changing to out-of-bag error\n",
    "    print mean_ndcg(clf, X_test, y_test)\n",
    "    print classification_report(clf.predict(X_test), y_test)\n",
    "#     df.loc[[n],'train_score'] = clf.oob_score_\n",
    "#     df.loc[[n],'test_score'] = recall_score(clf.predict(X_test), y_test, average='weighted')\n",
    "#     df.loc[[n],'ndcg_score'] = mean_ndcg(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         0\n",
      "          1       0.00      0.00      0.00         0\n",
      "          2       0.00      0.00      0.00         0\n",
      "          3       0.00      0.00      0.00         0\n",
      "          4       0.00      0.00      0.00         0\n",
      "          5       0.00      0.00      0.00         0\n",
      "          6       0.00      0.00      0.00         0\n",
      "          7       0.93      0.65      0.77     58855\n",
      "          8       0.00      0.00      0.00         0\n",
      "          9       0.00      0.00      0.00         0\n",
      "         10       0.31      0.55      0.39     11584\n",
      "         11       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.83      0.63      0.70     70439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_pred = clf.predict_proba(data[train.shape[0]:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ids = []  #list of ids\n",
    "#cts = []  #list of countries\n",
    "#for i in range(len(id_test)):\n",
    "#    idx = id_test[i]\n",
    "#    ids += [idx] * 5\n",
    "#    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "#sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "#sub.to_csv(os.path.join('data', 'sub.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.to_pickle('X.pkl')\n",
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_pickle('X.pkl')\n",
    "y = np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature selection, we run random forest and we remove some less-useful features\n",
    "# to improve performance\n",
    "clf = RandomForestClassifier(n_estimators=250, oob_score=True, n_jobs=1, criterion='entropy')\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\n",
    "                \"max_depth\": [16, 32, 64],\n",
    "#                 \"n_estimators\": [500]\n",
    "#               \"max_features\": [\"sqrt\", 'log2'],\n",
    "#               \"min_samples_split\": [2],\n",
    "#               \"min_samples_leaf\": [1, 3, 10],\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(clf, param_grid, scoring='f1_weighted', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = final[:train.shape[0]]\n",
    "search.fit(X, y)\n",
    "print search.best_params_\n",
    "print search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'grid_scores_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-137f763e603f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'grid_scores_'"
     ]
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
