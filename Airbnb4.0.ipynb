{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import os\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "test = pd.read_csv(os.path.join('data', 'test_users.csv'), header=0, parse_dates=[1,2,3])\n",
    "train = pd.read_csv(os.path.join('data', 'train_users_2.csv'), header=0, parse_dates=[1,2,3])\n",
    "df_sessions = pd.read_csv(\"data/sessions.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking care of Train + Test Users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encoding country destinations in train set\n",
    "outcome = train.country_destination\n",
    "labels = outcome.values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "train = train.drop(['country_destination'], axis=1)\n",
    "\n",
    "#storing user ids in test set\n",
    "id_test = test['id']\n",
    "\n",
    "#appending test to train and dropping date first booking which is redundant\n",
    "data = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "data = data.drop(['date_first_booking'], axis=1)\n",
    "\n",
    "#extracting features from date_account_created\n",
    "data['dac_year'] = data.date_account_created.apply(lambda x: x.year)\n",
    "data['dac_month'] = data.date_account_created.apply(lambda x: x.month)\n",
    "data['dac_weekday'] = data.date_account_created.apply(lambda x: x.weekday())\n",
    "data = data.drop(['date_account_created'], axis=1)\n",
    "\n",
    "#extracting features from timestamp_first_active\n",
    "data['tfa_year'] = data.timestamp_first_active.apply(lambda x: x.year)\n",
    "data['tfa_month'] = data.timestamp_first_active.apply(lambda x: x.month)\n",
    "data['tfa_weekday'] = data.timestamp_first_active.apply(lambda x: x.weekday())\n",
    "data = data.drop(['timestamp_first_active'], axis=1)\n",
    "\n",
    "#filling age nan with age median\n",
    "data.age = data.age.fillna(data.age.median())\n",
    "\n",
    "#binning age column \n",
    "bins = list(np.arange(15, 85, 5))\n",
    "bins.insert(0,0)\n",
    "bins.append(int(max(data.age)))\n",
    "group_names = ['<15', '15-20', '20-25', '25-30', '30-35', '35-40', '40-45', '45-50', \n",
    "               '50-55', '55-60', '60-65', '65-70', '70-75', '75-80', '>80']\n",
    "data['age_bucket'] = pd.cut(data['age'], bins, labels=group_names)\n",
    "\n",
    "#cleaning gender column and filling nan in all dataframe with 'unknown' \n",
    "data.gender = data.gender.replace('-unknown-','unknown')\n",
    "data.ix[:, data.columns != 'age_bucket'] = data.ix[:, data.columns != 'age_bucket'].fillna('unknown')\n",
    "\n",
    "#generating dummy variables in top of categorical columns\n",
    "to_be_dummified = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser','age_bucket']\n",
    "for f in to_be_dummified:\n",
    "    dummies = pd.get_dummies(data[f], prefix=f)\n",
    "    data = data.drop([f], axis=1)\n",
    "    data = pd.concat((data, dummies), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking care of Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop row with nan values from the \"user_id\" column as they're useless\n",
    "df_sessions = df_sessions.dropna(subset=[\"user_id\"])\n",
    "\n",
    "# Frequency of devices - by user\n",
    "device_freq = df_sessions.groupby('user_id').device_type.value_counts()\n",
    "\n",
    "# Frequency of actions taken - by user\n",
    "action_freq = df_sessions.groupby('user_id').action.value_counts()\n",
    "\n",
    "# Total list of users\n",
    "users = data.id\n",
    "\n",
    "def feature_dict(df):\n",
    "    f_dict = dict(list(df.groupby(level='user_id')))\n",
    "    res = {}\n",
    "    for k, v in f_dict.items():\n",
    "        v.index = v.index.droplevel('user_id')\n",
    "        res[k] = v.to_dict()\n",
    "    return res\n",
    "\n",
    "# Make a dictionary with the frequencies { 'user_id' : {\"IPhone\": 2, \"Windows\": 1}}\n",
    "action_dict = feature_dict(action_freq)\n",
    "device_dict = feature_dict(device_freq)\n",
    "\n",
    "# Transform to a list of dictionaries\n",
    "action_rows = [action_dict.get(k, {}) for k in users]\n",
    "device_rows = [device_dict.get(k, {}) for k in users]\n",
    "\n",
    "device_transf = DictVectorizer()\n",
    "tf = device_transf.fit_transform(device_rows)\n",
    "\n",
    "action_transf = DictVectorizer()\n",
    "tf2 = action_transf.fit_transform(action_rows)\n",
    "\n",
    "# Concatenate the two datasets\n",
    "# Those are row vectors with the frequencies of both device and actions [0, 0, 0, 2, 0, 1, ...]\n",
    "features = sp.hstack([tf, tf2])\n",
    "\n",
    "# We create a dataframe with the new features and we write it to disk\n",
    "df_sess_features = pd.DataFrame(features.todense())\n",
    "df_sess_features['id'] = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del df_sessions # Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Joining Train + Test Users with Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#left joining data and sessions on user_id\n",
    "final = pd.merge(data, df_sess_features, how='left', left_on='id', right_on='id')\n",
    "final.ix[:, final.columns != 'age_bucket'].fillna(-1, inplace=True)\n",
    "\n",
    "# Using inplace because I have 8GB of RAM\n",
    "# final.ix[:, final.columns != 'age_bucket'] = final.ix[:, final.columns != 'age_bucket'].fillna(-1)\n",
    "\n",
    "final.drop(['id'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mylib.scoring import ndcg_at_k\n",
    "\n",
    "def mean_ndcg(clf, X, y):\n",
    "    # Predict class probabilities\n",
    "    y_predict = clf.predict_proba(X)\n",
    "    # Get highest 5 predictions\n",
    "    best_5 = np.argsort(-y_predict, axis=1)[:, :5]\n",
    "    \n",
    "    # Transform to relevance scores\n",
    "    relevance = (best_5 == y[:, np.newaxis]).astype('int')\n",
    "    \n",
    "    # Calculate ndcg for each sample and take average (?)\n",
    "    return np.mean([ndcg_at_k(row, 5) for row in relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature importance\n",
    "clf = RandomForestClassifier(n_estimators=160, oob_score=True, n_jobs=1, criterion='entropy')\n",
    "X = final[:train.shape[0]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(clf.feature_importances_ < 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unimportant_features = clf.feature_importances_ < 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = final[:train.shape[0]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.ix[:, ~unimportant_features], y, test_size=0.33, random_state=42)\n",
    "\n",
    "n_trees = [10, 20, 40, 80, 160]\n",
    "df = pd.DataFrame(np.nan, index=n_trees, columns=['train_score', 'test_score', 'ndcg_score'])\n",
    "for n in n_trees:\n",
    "    clf = RandomForestClassifier(n_estimators=n, oob_score=True, n_jobs=1, random_state=42, criterion='entropy', max_depth=4)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    \n",
    "    # The RF will give you unusually high training error if calculated this way\n",
    "    # http://stats.stackexchange.com/questions/66543/random-forest-is-overfitting\n",
    "    \n",
    "    # Changing to out-of-bag error\n",
    "    df.loc[[n],'train_score'] = clf.oob_score_\n",
    "    df.loc[[n],'test_score'] = clf.score(X_test, y_test)\n",
    "    df.loc[[n],'ndcg_score'] = mean_ndcg(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>ndcg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.587853</td>\n",
       "      <td>0.589574</td>\n",
       "      <td>0.922014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.590510</td>\n",
       "      <td>0.584222</td>\n",
       "      <td>0.922020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.589755</td>\n",
       "      <td>0.583555</td>\n",
       "      <td>0.922020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.590048</td>\n",
       "      <td>0.584704</td>\n",
       "      <td>0.922020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.588860</td>\n",
       "      <td>0.583597</td>\n",
       "      <td>0.922020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_score  test_score  ndcg_score\n",
       "10      0.587853    0.589574    0.922014\n",
       "20      0.590510    0.584222    0.922020\n",
       "40      0.589755    0.583555    0.922020\n",
       "80      0.590048    0.584704    0.922020\n",
       "160     0.588860    0.583597    0.922020"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#y_pred = clf.predict_proba(data[train.shape[0]:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ids = []  #list of ids\n",
    "#cts = []  #list of countries\n",
    "#for i in range(len(id_test)):\n",
    "#    idx = id_test[i]\n",
    "#    ids += [idx] * 5\n",
    "#    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n",
    "\n",
    "#Generate submission\n",
    "#sub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\n",
    "#sub.to_csv(os.path.join('data', 'sub.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature selection, we run random forest and we remove some less-useful features\n",
    "# to improve performance\n",
    "clf = RandomForestClassifier(n_estimators=80, oob_score=True, n_jobs=1, criterion='entropy')\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\n",
    "                \"max_depth\": [1, 2, 4, 8, 10],\n",
    "                \"n_estimators\": [80]\n",
    "#               \"max_features\": [\"sqrt\", 'log2'],\n",
    "#               \"min_samples_split\": [2],\n",
    "#               \"min_samples_leaf\": [1, 3, 10],\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(clf, param_grid, scoring=mean_ndcg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 80, 'max_depth': 4}\n",
      "0.923523075753\n"
     ]
    }
   ],
   "source": [
    "# X = final[:train.shape[0]]\n",
    "search.fit(X_train, y_train)\n",
    "print search.best_params_\n",
    "print search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.92352, std: 0.00004, params: {'n_estimators': 80, 'max_depth': 1},\n",
       " mean: 0.92352, std: 0.00004, params: {'n_estimators': 80, 'max_depth': 2},\n",
       " mean: 0.92352, std: 0.00004, params: {'n_estimators': 80, 'max_depth': 4},\n",
       " mean: 0.92346, std: 0.00012, params: {'n_estimators': 80, 'max_depth': 8},\n",
       " mean: 0.92334, std: 0.00003, params: {'n_estimators': 80, 'max_depth': 10}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
